---
type: "quote"
title: "Scaling Laws and Compression in AI"
author: "Daniel Selsam"
url: "https://www.youtube.com/watch?v=6nJZopACRuQ"
date: "2025-04-11"
tags: ["LLMs"]
---

> This whole effort, which was hugely expensive in terms of people and time and dollars and everything else, was an experiment to further validate that the scaling laws keep going and why. And it turns out they do, and they probably keep going for a long time. I accept scaling laws like I accept quantum mechanics or something, but I still don't know why. Why should that be a property of the universe? So why are scaling laws a property of the universe?

> The fact that more compression will lead to more intelligence has this very strong philosophical grounding. So the question is: why does training bigger models for longer give you more compression? There are a lot of theories here. The one I like is that the relevant concepts are sort of sparse in the data of the world, and in particular, it's a power law. The hundredth most important concept appears in one out of a hundred of the documents, or whatever. So there's long tails.
>
> If we make a perfect dataset and figure out very data-efficient algorithms, can we go home? It means that there's potentially exponential compute wins on the table if you're very sophisticated about your choice of data. But basically, when you just scoop up data passively, you're going to require 10x-ing your compute and your data to get the next constant number of things in that tail. And that tail keeps goingâ€”it's long. You can keep mining it, although you can probably do a lot better.
>
> I think that's a good place to leave it. Thank you guys very much, that was fun.

Source Pre-Training GPT-4.5: [YouTube](https://www.youtube.com/watch?v=6nJZopACRuQ)
