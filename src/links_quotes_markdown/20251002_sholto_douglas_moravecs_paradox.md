---
type: "quote"
author: "Sholto Douglas"
date: "2025-10-02"
url: "https://youtu.be/FQy4YMYFLsI?si=Ao-7mP1qft0l2s3d&t=1125"
tags: ["robotics", "reinforcement-learning"]
---

> There’s this thing called Moravec’s paradox — the idea that tasks humans find easy, like manipulating or picking up objects, are hard for AI, while things we find hard, like reasoning through mathematical problems, are easy.
>
> I actually think Moravec’s paradox is a bit misleading. It’s mostly about data availability and reinforcement learning signals. Take robotic locomotion, for example — robots’ ability to walk and balance. If you look at the latest videos from Unitree robots, the difference between now and two years ago is dramatic. They’re incredibly agile; there’s even a video where one gets kicked over and pops back up like something out of The Matrix.
>
> This progress is because locomotion provides a simple and strong reinforcement learning signal. It’s largely solved with basic RL now. Manipulation is harder, but I’m still optimistic about robotics for several reasons.
>
> First, I’ve seen remarkable progress from robotics labs this year — they can now handle fairly complex physical tasks. Second, there’s what I’d call a large generator–verifier gap. Improving language models requires finding humans who can outperform them, but in robotics we can use general models as teachers or judges. For example, if a robot is told to “stack the red block on top of the blue block,” a language model can evaluate whether it succeeded and provide a reward accordingly.
>
> Finally, for a long time people thought robotics required solving long-term coherence and planning, but language models have made that easier by breaking tasks into smaller steps. Most robotics labs are now focused on refining motor policies, and they’re making incredible progress. It’s mainly a question of improving data and feedback loops.
