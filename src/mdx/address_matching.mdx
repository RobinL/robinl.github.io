---
title: "Building Accurate Address Matching Systems"
post_date: "2025-06-13"
post_category: "data"
description: ""
code_url: "https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/address_matching.mdx"
prob_linkage_category: "address_matching"
tutorial_number: 1
date: "2025-06-13"
---

export { MDXLayout as default } from '../components/MDXLayout';
import { SEO } from "../components/SEO"
import Subtitle from "../components/Subtitle.jsx"
import { Link } from "gatsby"
import AddressComparison from "./address_matching/AddressComparison.jsx"
import AddressCandidatesComparison from "./address_matching/AddressCandidateComparison.jsx"




export const Head = ( props ) => <SEO frontmatter={props.pageContext.frontmatter} />;

# Building Accurate Address Matching Systems

<Subtitle>A bag of tricks to improve the accuracy of geocoding</Subtitle>

## The challenge


Address matching (geocoding) is a notoriously difficult problem due to unpredictable structure of addresses and the many different ways an address can be written.

Substantial variation can exist between two matching addresses - for instance:

<AddressComparison
  addressA={<><mark>Flat</mark> 165 <mark>Block 3</mark> Philpot Square, <mark>Hammersmith And Fulham</mark></>}
  addressB={<>165, Philpot Square, <mark>London</mark></>}
  isMatch={true}
/>




Whereas the following example has far less variation, yet does not match:

<AddressComparison
  addressA={<>Flat A 2<mark>4</mark> Jubilee Street, London, LO1 23D</>}
  addressB={<>Flat A 2<mark>5</mark> Jubilee Street, London, LO1 23D</>}
  isMatch={false}
/>

The challenge is therefore to develop an algorithm that can somehow 'see' that the first pair of addresses are more similar than the second pair.


Unfortunately one of the most effective general approaches to record linkage – known as the <Link to="/intro_to_probabilistic_linkage/">Fellegi-Sunter model</Link> – is not well suited to address matching because:
- there is correlation between different parts of an address, violating the model's statistical assumptions
- the model assumes input data is split across multiple columns (e.g. first name, surname, DoB etc.), but address data is often received as a single string, and reliably parsing this into a set of standardised columns is very difficult


In this post, I share some tricks and feature engineering techniques we can use to exploit the information in addresses as much as possible, to maximise geocoding accuracy.

Concrete implementations of some these tricks can be found in two open source address matching libraries:
 -  [`uk_address_matcher`](https://github.com/RobinL/uk_address_matcher)<span style={{marginLeft: '0.25em', verticalAlign: '0.0em'}} aria-label="external link" title="External link"><svg width="16" height="16" viewBox="0 0 24 24" aria-hidden="true"><path d="M14 3h7v7h-2V6.414l-9.293 9.293-1.414-1.414L17.586 5H14V3z" fill="#2563eb"/><path d="M5 5h5v2H7v10h10v-3h2v5H5V5z" fill="#2563eb"/></svg></span> a [Splink](https://github.com/moj-analytical-services/splink) based address matching library focussed on UK addresses
 -  [`whereabouts`](https://github.com/ajl2718/whereabouts)<span style={{marginLeft: '0.25em', verticalAlign: '0.0em'}} aria-label="external link" title="External link"><svg width="16" height="16" viewBox="0 0 24 24" aria-hidden="true"><path d="M14 3h7v7h-2V6.414l-9.293 9.293-1.414-1.414L17.586 5H14V3z" fill="#2563eb"/><path d="M5 5h5v2H7v10h10v-3h2v5H5V5z" fill="#2563eb"/></svg></span>, a duckdb-powered address matching library, initially focussed on Australian addresses, but [now supporting any addresses?]

# Structuring the problem

I will assume that we have:
- A list of messy addresses (e.g. addresses that have been entered by hand by users)
- A canonical address file (e.g. in the UK, an Ordnance Survey product such as [Built Address](https://docs.os.uk/osngd/data-structure/address/gb-address))

To be as general as possible, we'll assume the address in these files just is a single string.

The address matching problem can be structured into two main stages:
- [blocking](https://moj-analytical-services.github.io/splink/topic_guides/blocking/blocking_rules.html?h=blocking#blocking) (finding a small number of plausible candidates from the canonical address file)
- scoring (to rank the candidates and thereby identify the best match).

These steps are indpendent, so we can mix and match different blocking and scoring techniques.

However, prior to either of these steps, we need to decide how best to represent the address data.


## How to represent the address data
A tempting first step is to attempt to parse the messy addresses semantically - e.g. splitting out flat number, house number, street name and so on.[^1]

[^1]: A [Conditional Random Fields](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsworkingpaperseriesno17usingdatasciencefortheaddressmatchingservice) or [hidden Markov](https://pmc.ncbi.nlm.nih.gov/articles/PMC140019/) model could be used for this, or existing libraries such as [libpostal](https://github.com/openvenues/libpostal).

This is appealing because it appears to enable powerful scoring rules, such as 'if the building number is different, the addresses do not match'.

In practice, this approach suffers from a paradox: the hardest addresses to match often contain ambiguities which make them the hardest to parse.

The `Philpot Square` addresses shown above is a good practical example:  how should we interpret the `165` in `165, Philpot Square, London`?  In difficult examples like this, solving this problem amounts to figuring out what the true address is - so the problem is circular.

As a result, it may be better to treat the address as a single string from which we can extract certain features such as 'the first number', without attaching too much semanitic meaning.

This prevents us being too rigid later in our blocking and scoring approaches - for example, it avoids the trap of logic like 'if the building number is different it's not a match' - which contains an implicit assumption of complete information that's correctly parsed.

It also gives us greater flexibility in feature engineering, because we can find features that have no particular structural or semantic meaning but are nonetheless useful for matching.

## Blocking stage

The purpose of blocking is to recover a list of candidate addresses.  The aim of the blocking stage is to have high recall (i.e. not to miss the true match).  Precision is less important, because we will be scoring the candidates to find the true match at the next stage.

For example, a simple blocking strategy could be to simply find all addresses in the postcode of the query address[^2].  This is a good start, but in practice suffers two flaws:
- The list of candidates can be excessively long, resulting in slower performance
- The query address may not have a postcode, or the postcode may be incorrect

[^2]: Blocking on postcode only may result in a list of candidates that is too long for performance reasons. In practice, a simple extension could be to limit to addresses that contain the postcode and any of the numbers in the address.

As a result we need to supplement a postcode-based blocking strategy with some additional blocking techniques.  The idea is that any of these techniques in isolation may miss the true match, but the  combined results should almost always contain it.

### 1. Surmounting problems of common words: n-grams combined with term frequencies

We can pre-processes addresses to extract n-grams and their associated term frequencies (frequency of occurrence across the corpus of all addresses).   This allows us to identify which tokens are uncommon enough to be useful for blocking (i.e. will restrict the number of candidates to a manageable number).

Many strategies are possible here, but a simple approach could be to identify the least common bigram or trigram in each address, and to block on that.  For performance reasons, we'd want to filter out any excessively common ones.

For example each individual token in `Flat A 24 Jubilee Street London` is very common, but the tri-gram `A 24 Jubilee` will appear in at most a handful of addresses in the country.

### 2. Word order: Single words sorted by term frequency

A variation on the n-gram approach is to extract the most unusual single tokens (words) in the address, and block on (say) the two least common words in the address.

This is particularly useful to avoid the problem of missing or misordered tokens.  For example, in the Philpot Square example, `Philpot` and `165` are unusual tokens and there may only be a single address in the country with both tokens, but they don't predictably appear next to each other.

### 3.  Discriminating tokens amongst neighbouring addresses

By working backwards and eliminating common tokens, we can identify the following discriminating tokens (highlighted):

<pre style={{
  display: 'inline-block',
  width: 'max-content',
  textAlign: 'right',
  background: 'none',
  border: 'none',
  padding: 0
}}>
  <pre><code><mark>1</mark> Rainbow Lane Taunton TA1 1AB</code></pre>
  <pre><code><mark>2</mark> Rainbow Lane Taunton TA1 1AB</code></pre>
  <pre><code><mark>Highfield</mark> Rainbow Lane Taunton TA1 1AB</code></pre>
  <pre><code><mark>Old Station House</mark> Rainbow Lane Taunton TA1 1AB</code></pre>
  <pre><code><mark>5</mark> Rainbow Lane Taunton TA1 1AB</code></pre>
</pre>

These highlighted n-grams can be used for blocking and can be especially used for non-numeric house names, combined with (say) the first half of the postcode.

### 4. Deriving multiple variations of the canonical address

In many cases, the list of canonical addresses will be high quality data split robustly and semantically across multiple columns ((e.g. see Built Adddress spec [here](https://docs.os.uk/osngd/data-structure/address/gb-address/built-address)).

If so, we can derive multiple representations of the canonical address by combining these columns in different ways.  This gives a greater chance for blocking to recover the true match, and also means scoring will be more reliable because it increases the similarity between the messy address and the closest representation of the canonical address.

For example, the 'standard' representation may not contain 'floor descriptors' like 'top floor', but if this is sometimes observed in the messy addresses, we could add this as a variation on to the canonical address.

This adds little complexity:  we can treat the variations as additional candidates in the blocking stage, and then simply take the highest score for each candidate in the scoring stage.

This technique also increases the change of finding an exact match, so the run time performance tradeoff is not unambiguously negative.






## Scoring stage

The purpose of the scoring stage is to identify the highest scoring candidate found in the blocking stage, to try to identify the correct match.

There are many different scoring approaches, and a weighted combination of approaches will often yield the best results.


### 1. Token frequencies within the corpus of all addresses

A simple 'first cut' of a scoring approach can be simply to look at the number of common tokens/words between the messy address and candidate address, weighted by token frequency in the corpus of all addresses.

For example:

<AddressCandidatesComparison
  messyAddress={<><mark>24</mark> <mark>Jubilee</mark> St, <mark>London</mark></>}
  candidates={[{
    address: <><mark>24</mark> <mark>Jubilee</mark> Street, <mark>London</mark> LO1 23D</>,

  }]}
/>

| Overlapping token | Share of all addresses containing token|
|-------------------|---------------------------------------|
| <mark>24</mark>              | 0.15 % |
| <mark>Jubilee</mark>         | 0.003 % |
| <mark>London</mark>          | 5.0 % |

0.0015 × 0.00003 × 0.05 ≈ 2.25 × 10⁻⁹


This is surprisingly effective, but there are several drawbacks that mean it is possible to improve on this approach:
- Token frequency in the corpus of all addresses in not a very good metric of the strength of match within a small group of candidates.  For instance, the token 'London' is very common in the overall corpus, but for an address in Manchester, it may be highly discriminative.
- Similarly, a token may be very uncommon in the overal corpus, but useless to distinguish between candidates.  For example, consider the token 'Rainbow' in the above list of addresses.
- Sometimes the true match may have fewer overlapping tokens than a false match.  For example, in the `Philpot Square` example, a different and incorrect candidate may include the tokens `Block 3`

### 2. Token (or n-gram) frequencies within candidates

An improvement on using whole-corpus token frequencies is to instead use the frequencies of tokens within the set of candidates.  This is a better measure of the power of the token to distinguish between the candidates.

Similarly, we can use the frequency of n-grams within the set of candidates.

### 4. Absence of tokens:  Matching tokens that exist within other candidates, but not the present candidate

Sometimes a token (or n-gram) from the messy address will be present in _other_ candidates, but no in the one being scored.  This is a strong indicator _against_ the candidate being the true match.

### Scoring stage additional considerations

One drawback of techniques (2) and (3) the n-gram approaches to blocking tends to favour a list of candidates that contain discriminative tokens.  For example, if we'd used `1 Coronation Street` as a tri-gram for blocking, we'd have retrieved several different `1 Coronation Street` candidates!




### 3. Discriminating tokens amongst neighbouring addresses

We can use the technique desribed above (Rainbow Lane example) to identify discriminating tokens amongst neighbouring addresses.

On the face of it, these tokens are essential information to uniquely pin down the address, and so a match should require a match on these tokens.

In practice, addresses are not this simple,

[Example e.g. 'top floor']

As a result, instead of requiring a match, we can simply place greater weight on these tokens when scoring.

### 4. Distinguishability of the top scoring address




Some tokens or n-grams may be common across the corpus of all addresses, but rare within the set of candidates.

For instance, suppose we have the following messy address:

Messy address:
`### Highly-discriminative tokens within candidates, London`

And candidates:
`1 Coronation Street, London`
`2 Coronation Street, London`
`3 Coronation Street, London`

Then the token `1` is unique (highly discriminative) amongst the candidate, despite being a common token across the corpus of all addresses.

A match on a highly discriminative token is a strong, but not definitive, indicator that the candidate is the correct match.

### Highly-discriminative amongst neighbouring addresses

A drawback of using the list of candidates to identify discriminative tokens is that

A second approach to identifying discriminative tokens is to look at the candidates in the context of their neighbours.  If we sort addresses by the reverse of the full address string, we can remove common tokens.  For instance:


```
24 Jubilee Street, London, LO1 23D
25 Jubilee Street, London, LO1 23D
Flat A 25 Jubilee Street, London, LO1 23D
Flat B 25 Jubilee Street, London, LO1 23D
```

Can be used to pull out

```
24
NULL
Flat A
Flat B
```

During scoring, this can be used to determine that - for example - the number `25` does not uniquely determine the match, but `24` does.

## Scoring stage: Interpreting the score



### Distinguishability of the top scoring address





## Data Cleaning Stage








## Further reading

- A variety of interesting edge casees can be found in [Falsehoods Programmers Believe About Addresses](https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/)


### Some examples that highlight the challenges of address matching


<AddressComparison
  addressA={<>23A Marchant House, Jubilee Street, London, LO1 23D</>}
  addressB={<>Flat A, Top Floor, Marchant House, 23 Jubilee Street, Fulham, LO1 23D</>}
  isMatch={true}
/>

<AddressComparison
  addressA={<>FLAT 3 ST LEGER HOUSE GREAT LINFORD MK14 5HA</>}
  addressB={<>3 ST LEGER HOUSE 4A ST LEGER COURT GREAT LINFORD MILTON KEYNES MK14 5HA</>}
  isMatch={true}
/>

<AddressComparison
  addressA={<>FLAT 62 BLOCK 3 PHILPOT SQUARE SW6 3HX</>}
  addressB={<>62 PHILPOT SQUARE LONDON SW6 3HX</>}
  isMatch={true}
/>

<AddressComparison
  addressA={<>6, PADDOCK CLOSE CASTLETHORPE MK19 7AY</>}
  addressB={<>PIPIT HOUSE, 6, PADDOCK CLOSE, CASTLETHORPE MK19 7AY</>}
  isMatch={true}
/>

<AddressComparison
  addressA={<>Maisonette First And Second Floors, 14, Hadyn Park Road</>}
  addressB={<>Top Floor Flat 14 Hadyn Park Road, London</>}
  isMatch={true}
/>

<AddressComparison
  addressA={<>Flat 39 Evans House White City Estate</>}
  addressB={<>39, EVANS HOUSE, AUSTRALIA ROAD, LONDON</>}
  isMatch={true}
/>






<AddressCandidatesComparison
  messyAddress={<>FLAT 3 ST LEGER HOUSE GREAT LINFORD MK14 5HA</>}
  candidates={[
    {
      address: <>GREAT LINFORD HOUSE 1 ST LEGER COURT GREAT LINFORD MK14 5HA</>,
      isMatch: false,
    },
    {
      address: (
        <>
          3 ST LEGER HOUSE <mark>4A</mark> ST LEGER COURT GREAT LINFORD MK14 5HA
        </>
      ),
      isMatch: true,
    },
    {
      address: <>3 ST LEGER COURT GREAT LINFORD MK14 5HA</>,
      isMatch: false,
    },
  ]}
/>

