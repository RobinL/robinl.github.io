import { Vega, VegaLite } from "react-vega"

import roc_comparison_cluster from "./comparing_splink_models/roc_comparison_trained_block_cluster.vl.json"
import roc_comparison_no_cluster from "./comparing_splink_models/roc_comparison_trained_block_no_cluster.vl.json"

# Are more complex probabilistic linkage models more accurate?

<p style="font-size: 1.2rem; color: #7c7c7c; margin-top: -10px">
  <i>Part 2: Unsupervised learning </i>
</p>

## Introduction

In part 1 of this post, I showed that more complex probabilistic linkage models can be dramatically more accurate than simple models. I used supervised learning to compute the model parameters directly from ground truth data to ensure a fair comparison. This meant that differences in performance could be attributed solely to the different model formulations, rather than anything to do with the process of estimation.

In part 1, I used the labelled data to ensure that match scores were computed for all true matches in the ground truth, in addition to being computed for non-matching comparisons that met a [list of blocking rules](https://github.com/moj-analytical-services/splink_synthetic_data_performance_comparisons/blob/8daa5dbddae7079c2569609a00c57fd0c7c60559/model_training/person/uk_citizens_max_groupsize_20/compare_models/00_estimate_m_u/glue_py_resources/splink_settings.py#L52). This meant that the results were not strongly dependent on the blocking approach.

In real applications of record linkage, we typically do not have ground truth data. This post explores whether the findings change if the same models are trained using unsupervised learning. Specifically, model parameters will be estimated using [Splink](https://github.com/moj-analytical-services/splink).

I find that the gains in accuracy from more complex models are similar, and seem little affected by the differences between estimated parameters and their true values.

I also find that convergence of the Expectation Maximisation algorithm is significantly faster for more complex models. This is not surprising: a more complex model more accurately describes the data, leading to more precise predictions on each iteration of the algorithm.

I also find evidence that the more accurate models seem to have greatest advantage when predicting the match status of some of the most hard-to-find matches. This may imply that, in models where blocking rules are very tight, the more complex models have less of a comparative advantage. That would make intuitive sense, since tight blocking rules pick out easier-to-match record comparisons, many of which are likely to be easy to match even with a very simple model.

## Results

First I present ROC curves on the same basis as in Part 1, but using the estimated parameters. For this chart, the ground truth data has been used to ensure that all true matches are included in the ROC curve. The blocking rules for this ROC curve are [here](https://github.com/moj-analytical-services/splink_synthetic_data_performance_comparisons/blob/8daa5dbddae7079c2569609a00c57fd0c7c60559/model_training/person/uk_citizens_max_groupsize_20/compare_models/00_estimate_m_u/glue_py_resources/splink_settings.py#L52).

<VegaLite spec={roc_comparison_cluster} />

Second, I present a ROC curve where record comparisons are generated from a long list of blocking rules. This is a better approximation of a real-life record linkage scenario, in which we have to use blocking rules to try and recover all positive matches.

<VegaLite spec={roc_comparison_no_cluster} />

## Methodology

Model training was performed using Splink. Specifically, I estimated u values directly, fix them, and then used the expectation maximisation algorithm to compute m values and the proportion of matches parameter.

## Other findings

It is interesting to compare the parameter estimates from the
