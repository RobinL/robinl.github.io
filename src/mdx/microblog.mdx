---
title: "Microblog"
post_date: "2025-01-16"
post_category: "quotes_links"
description: "My microblog"
code_url: "https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/microblog.mdx"
---

export { MDXLayout as default } from '../components/MDXLayout';
import { SEO } from "../components/SEO"
import Microblog from "../components/Microblog"
import { Link } from "gatsby"

export const Head = ( props ) => <SEO frontmatter={props.pageContext.frontmatter} />;

# Microblog


<Microblog
    title="LLMs getting better at coding"
    date="2025-02-05"
    tags={["llms", "coding", "ai"]}
>

[Latest Lex episode at 02:43:31](https://lexfridman.com/deepseek-dylan-patel-nathan-lambert-transcript/#:~:text=It%E2%80%99s%20even%20less%20prevalent) has a great section on why chain-of-thought matters.

It hit me very similar to when I [first heard](https://youtu.be/U5OD8MjYnOM?si=J9rACnXmZH2HDC9R&t=6055) ([transcript](https://v1.transcript.lol/read/youtube/%40lexfridman/6522f348033150beacd16e2a?part=1#:~:text=And%20I%20think%20there%27s%20a%20lot%20of%20fundamental%20questions) ) this idea that code generation abilities would improve faster than natural language.  It feels like potentially chain-of-thought is the key to making this work.

These two Kapathy tweets also very relevant:
[tweet1](https://x.com/karpathy/status/1883941452738355376) [tweet2](https://x.com/karpathy/status/1885026028428681698)

I'm not expecting to be manually writing code for much longer!!

</Microblog>

<Microblog
    title="Using DuckDB in ChatGPT Code Interpreter"
    date="2024-01-28"
    tags={["duckdb", "chatgpt", "tips"]}
>
You can use DuckDB in ChatGPT's code interpreter by providing this specific wheel file:

`duckdb-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`

from [here](https://pypi.org/project/duckdb/#files).

If you encounter any issues, you can copy paste the available wheels into ChatGPT and ask it to analyse which is the most likely to work.

For Altair, you need:
[narwhals-1.24.1-py3-none-any.whl](https://files.pythonhosted.org/packages/68/0e/882f7c0e073bf1f310dce159af6186826ca9b8ee7c170771c23e52a373dc/narwhals-1.24.1-py3-none-any.whl)
then
[altair-5.5.0-py3-none-any.whl](https://files.pythonhosted.org/packages/aa/f3/0b6ced594e51cc95d8c1fc1640d3623770d01e4969d29c0bd09945fafefa/altair-5.5.0-py3-none-any.whl)

Note: I could not get png generation to work because the `vl_convert_python` wheel would not install.
</Microblog>

<Microblog
    title="Using a uv shebang line"
    date="2024-01-28"
    tags={["python", "uv", "tips"]}
>
Rob Allen says:
> I create a fair few scripts in my ~/bin/ directory to automate tasks. Since discovering [uv and inline script metadata](https://akrabat.com/defining-python-dependencies-at-the-top-of-the-file/), Iâ€™ve started using Python far more for these.
>
>
> As `~/bin` is on my path, I want to run the script by calling it directly on the command line. To do this, I use this shebang:
>
> ```bash
> #!/usr/bin/env -S uv run --script
> ```
>


Full article [here](https://akrabat.com/using-uv-as-your-shebang-line/).
</Microblog>


<Microblog
    title="ClickHouse Obfuscator - Data Anonymization Tool"
    date="2024-01-16"
    tags={["clickhouse", "data", "privacy"]}
>
The [ClickHouse obfuscator](https://clickhouse.com/docs/en/operations/utilities/clickhouse-obfuscator) is a tool for anonymizing production data while preserving its key statistical properties. It maintains:

- Value cardinalities and distributions
- Data compression ratios
- String lengths and UTF-8 validity
- Time series continuity

See also [here](https://github.com/ClickHouse/ClickHouse/tree/master/programs/obfuscator)
</Microblog>


<Microblog
    title="National Data Library thoughts"
    date="2024-01-16"
    tags={["data", "governance", "AI"]}
>
Three key priorities for building a National Data Library:

1. Data sharing and governance must come first - before any technical solutions. Success depends on data owners being convinced the NDL will make their lives easier, not harder.

2. Access permissions and management systems need to be rock-solid before building analytical capabilities. Get the foundations right.

3. Design for an AI-first future - by the time the NDL is delivered, most analysis will likely be AI-assisted. The architecture needs to anticipate this.
</Microblog>

<Microblog
    title="Understanding DuckDB Connection Types in Python"
    date="2024-01-16"
    tags={["python", "duckdb", "database"]}
>
DuckDB offers special connection types in Python:

- `:memory:name` - Creates/connects to a named in-memory database that can be shared across connections
- `:default:` - Uses the default connection stored in the DuckDB module

Example:
```python
import duckdb

# Create table in default connection
duckdb.execute("CREATE TABLE tbl AS SELECT 42 as value")

# Access same table through explicit default connection
con = duckdb.connect(":default:")
con.sql("SELECT * FROM tbl")  # Works!

# Shared named memory connection
con3 = duckdb.connect(":memory:shared_db")
con4 = duckdb.connect(":memory:shared_db")  # Same database as con3
```

See docs [here](https://duckdb.org/docs/api/python/dbapi.html)
</Microblog>

<Microblog
    title="Configuring Python Path Visibility in VS Code interactive window (Jupyter)"
    date="2024-01-16"
    tags={["vscode", "jupyter", "tips"]}
>
When working with Jupyter notebooks in VS Code, add these settings to your `.vscode/settings.json`:
```json
{
    "jupyter.notebookFileRoot": "${workspaceFolder}",
    "python.analysis.extraPaths": [
        "${workspaceFolder}"
    ]
}
```

Now, the interactive window will run any script as if it's running from the root directory even if it's nested
</Microblog>

