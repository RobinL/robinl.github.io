---
title: "Splink 3: Fast, accurate and scalable linkage in Python"
description: "Splink 3 now offers support for Python and AWS Athena backends, in addition to Spark. It's now easier to use, faster and more flexible, and can be used for close to real time linkage."
post_date: "2022-08-05"
code_url: "https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/introducing_splink.mdx"
post_category: "data"
---


export { MDXLayout as default } from '../components/MDXLayout';
import { SEO } from "../components/SEO"
export const Head = ( props ) => <SEO frontmatter={props.pageContext.frontmatter} />;


import Subtitle from "../components/Subtitle"



# Splink 3: Fast, accurate and scalable data deduplication and record linkage in Python

<Subtitle>
  Splink now includes Python and AWS Athena backends, in addition to Spark. It's
  easier to use, faster and more flexible, and can be used for close to real
  time linkage.
</Subtitle>

Two years ago, [we introduced Splink](https://www.robinlinacre.com/introducing_splink/), a Python library for data deduplication and linkage (entity resolution) at scale.

![Splink banner](../images/splink_banner.png)

Since then, Splink has been used in government, the private sector, and academia to link and deduplicate huge datasets, some in excess of 100 million records, and it’s been downloaded over 3 million times.

We believe that it is now the fastest and most accurate free library for record linkage at scale.

We’ve learned a lot from our users, and we’ve just released version 3, which includes frequently requested features and tries to address some the biggest pain points.

## Major new features

- **Splink 3 now offers support for** [**Python and AWS Athena backends, in addition to Spark**](https://moj-analytical-services.github.io/splink/topic_guides/backends.html)**.** Linking in Python is supported by the Python [DuckDB](https://pypi.org/project/duckdb/) package, which is capable of linking datasets of up to about 2 million records on a laptop.
- **Small linkages can run 100x faster** through use the new DuckDB backend, relative to Spark. It’s possible to link a dataset of a million records in under two minutes on a modern laptop.
- [**Close to real time linkage is possible**](https://moj-analytical-services.github.io/splink/demos/real_time_record_linkage.html), using the DuckDB backend, which could enable Splink to be embedded in search services.
- [**Interactive dashboards**](https://moj-analytical-services.github.io/splink/demos/04_Visualising_predictions.html) are now core Splink outputs.
- Splink 3 is written to be agnostic to the target SQL backend, meaning adding support for new backends is [relatively straightforward](https://github.com/moj-analytical-services/splink/blob/master/splink/duckdb/duckdb_linker.py).

## Other changes

- **Linkage models are now simpler to estimate.** [Less code is needed,](https://moj-analytical-services.github.io/splink/index.html#quickstart) and the [API](https://moj-analytical-services.github.io/splink/linker.html) is clearer. Splink now automatically combines parameter estimates from different model runs, significantly reducing the number of lines of code to estimate a full model.
- **Linkages in Spark run significantly faster,** and are less likely to result in out of memory errors or other scaling issues. Big data linkage in Athena runs significantly faster than Spark in some instances.
- [**Clustering of links to produce unique identifiers**](https://moj-analytical-services.github.io/splink/linkerpred.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold) is now implemented in Splink, so does not require configuration of external dependencies like `graphframes`.
- [**A new documentation website**](https://moj-analytical-services.github.io/splink) brings together tutorials, topic guides, API documentation, and an interactive settings editor.
- **An overhaul of all** [**charting**](https://moj-analytical-services.github.io/splink) [**outputs**](https://moj-analytical-services.github.io/splink) makes them simpler, easier to read, and improves their labelling.

![Splink charts](../images/splink_charts.png)

## Try it out

You can try out the library in a Jupyter notebook in Binder [here](https://mybinder.org/v2/gh/moj-analytical-services/splink_demos/master?urlpath=lab). Note this is a small, free server, so don’t expect fast performance.

## Learn more

Please see our [series of tutorials](https://moj-analytical-services.github.io/splink/demos/00_Tutorial_Introduction.html), an [introductory video,](https://www.youtube.com/watch?v=msz3T741KQI) and our [interactive training materials on the underlying statistical theory](https://www.robinlinacre.com/probabilistic_linkage/).

## Feedback and contributing

We are grateful to all our users who have contributed and provided feedback. Please continue to do so by:

- Raising [an issue](https://github.com/moj-analytical-services/splink/issues) if you’ve found a bug or would like to request a new feature
- Starting a [discussion](https://github.com/moj-analytical-services/splink/discussions) if you have questions about how to do something
- Raising a [pull request](https://github.com/moj-analytical-services/splink/pulls) if you’d like to fix a bug or add a feature

Or I’m [@robinlinacre](https://twitter.com/RobinLinacre) on Twitter.

We’d be particularly interested to work with anyone who’d like to add support for new SQL backends such as Postgres, Google BigQuery etc.

