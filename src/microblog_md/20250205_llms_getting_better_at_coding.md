---
title: "LLMs getting better at coding"
date: "2025-02-05"
tags: ["LLMs"]
---

[Latest Lex episode at 02:43:31](https://lexfridman.com/deepseek-dylan-patel-nathan-lambert-transcript/#:~:text=It%E2%80%99s%20even%20less%20prevalent) has a good section on why chain-of-thought matters over and above just 'it increases the benchmark results'.

It hit me very similar to when I [first heard](https://youtu.be/U5OD8MjYnOM?si=J9rACnXmZH2HDC9R&t=6055) ([transcript](https://v1.transcript.lol/read/youtube/%40lexfridman/6522f348033150beacd16e2a?part=1#:~:text=And%20I%20think%20there%27s%20a%20lot%20of%20fundamental%20questions)) the idea that code generation abilities would improve faster than natural language. It feels like potentially chain-of-thought is the key to making this work.

These two Kapathy tweets also very relevant:
[tweet1](https://x.com/karpathy/status/1883941452738355376) [tweet2](https://x.com/karpathy/status/1885026028428681698)

Seems quite likely we'll see superhuman coding abilities in the not too distant future.