---
title: "Super-fast deduplication of large datasets using Splink and DuckDB"
description: "Interactive, explorable explanations of the Fellegi Sunter model, providing an introduction to probabilistic record linkage (data deduplication)."
post_date: "2023-01-15"
post_category: "probabilistic_linkage"
code_url: "https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/fast_deduplication.mdx"
prob_linkage_category: "tutorial"
---


export { MDXLayout as default } from '../components/MDXLayout';
import { SEO } from "../components/SEO"
export const Head = ( props ) => <SEO frontmatter={props.pageContext.frontmatter} />;
import { Link } from 'gatsby';
import { Vega, VegaLite } from "react-vega"
import summary_chart from "./fast_deduplication/summary_chart.vl.json"
import cpu_mem_chart from "./fast_deduplication/cpu_mem_chart.vl.json"

import Subtitle from "../components/Subtitle"

# Super-fast deduplication of large datasets using Splink and DuckDB

<Subtitle>Evaluating 1 billion record comparisons to deduplicate 8 million records in less than a minute</Subtitle>

## Summary

Data deduplication is a ubiquitous problem that results from multiple records being collected for the same entity:

| first_name | surname | dob        | city       | source_data_system |
| ---------- | ------- | ---------- | ---------- | ------------------ |
| lucas      | smith   | 1984-01-02 | London     | sales              |
| lucas      | smyth   | 1984-07-02 | Manchester | sales              |
| lucas      | smyth   | 1984-07-02 |            | marketing          |


The lack of a unique identifier means that there's no easy way of knowing whether these records refer to the same person.

[Splink](https://github.com/moj-analytical-services/splink) is a Python library for deduplicating and linking this kind of data. It's designed for use on very large datasets, so speed is imperative.  It  [DuckDB](https://duckdb.org/) as its default backend to achieve fast parallelised execution.

This blog post presents benchmarking results of Splink that show Splink is able to deduplicate an 8 million record dataset by evaluating 1 billion record comparisons in less than a minute, and at a cost of less than $1.00.

<VegaLite spec={summary_chart} />

To the best of my knowlege, these results show that it is fastest free tool for accurately deduplicating large datasets - by at least an order of magnitude.

Please <Link to="/about/">let me know</Link> if you think otherwise and I will update this post!



## The data

A dataset of historical persons was collected from [WikiData](https://www.wikidata.org/wiki/Wikidata:Main_Page) using the [Wikidata Query Service](https://query.wikidata.org/).

Each record was corrupted to generate between 1 and 20 duplicates using an xxxx distribution.  The average (mean) number of duplicates is xxxxx and the mean is xxxxxx.


A typical group of records looks like this:




## Methodology

Benchmarking was conducted on a variety of AWS EC2 instances using [pytest-benchmark](https://pypi.org/project/pytest-benchmark/).

I have been careful to report not just the number of input rows, but also the number of pairwise comparisons made, because performance is primarily driven by the later.

The number of comparisons is a user-configurable parameter (via blockign rules) and so it's always possible to 'cheat' and get a better runtimes simply by using stricter blocking, at the cost of lower accuracy.

In this case, I've chosen to make a relatively large number of comparisons per input record (around 100), which is representative of a real-world use case.

## Comparisons to other libraries

A chart of runtimes three other popular open source record linkage/data deudplication packages can be found in [this paper](https://imai.fas.harvard.edu/research/files/linkage.pdf) from the authors of the [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf) R package.

The chart shows fastLink has by far the best performance, at 400 minutes to deduplication 300,000 records on an 8 core machine.

Splink is able to deduplicate the same number of records in less than a minute on a comparable machine, suggesting it's at least two orders of magnitude faster.

A caveat is that these runtimes are not strictly comparable because the input datasets are not the same.  However, the size of the differences are so large that it's almost certain that Splink is a great deal faster.


## Accuracy

This post considers runtime only.  Accuracy statistics would only be valid on the specific datset used for benchmarking, and I do not have comparative accuracy statistics for other libraries.

However, there are theoretical and empircal reasons to believe Splink offers accuracy comparable to or better than alternaties:

- First, Splink is able to effectively harness most of the information in the input data in making a prediction - as explained in <Link to="/fellegi_sunter_accuracy/">this post</Link>.
- Second, an [independent empircal study](https://scholar.googleusercontent.com/scholar?q=cache:o4-c8w6DveYJ:scholar.google.com/+splink&hl=en&as_sdt=0,5&as_ylo=2023) have found Splink's accuracy compares favourably to alternatives.

I hope to write another post at some point looking at comparative accuracy in further details


[link off to accuracy blog posts here rather thank quoting accuracy]

## Memory and CPU usage

The following chart shows CPU and memory usage for each run.  Click on the bar chart to change which machine type (instance) is selected.

<VegaLite spec={cpu_mem_chart} />

## Model details

<details>
<summary>Click to expand</summary>

```python

import splink

```

</details>

## Further reading

## Reproducuction materials

Materials to reproduce the results in this post can be found in the following two repos:
- [Code to set up AWS infrastructure](https://github.com/RobinL/benchmarking_splink_infrastructure)
- [Code to run the benchmarks](https://github.com/RobinL/run_splink_benchmarks_in_ec2)
