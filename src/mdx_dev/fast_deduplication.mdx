---
title: "Super-fast deduplication of large datasets using Splink and DuckDB"
description: "Interactive, explorable explanations of the Fellegi Sunter model, providing an introduction to probabilistic record linkage (data deduplication)."
post_date: "2023-01-15"
post_category: "probabilistic_linkage"
code_url: "https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/fast_deduplication.mdx"
prob_linkage_category: "tutorial"
---


export { MDXLayout as default } from '../components/MDXLayout';
import { SEO } from "../components/SEO"
export const Head = ( props ) => <SEO frontmatter={props.pageContext.frontmatter} />;
import { Link } from 'gatsby';
import { Vega, VegaLite } from "react-vega"
import summary_chart from "./fast_deduplication/summary_chart.vl.json"

import Subtitle from "../components/Subtitle"

# Super-fast deduplication of large datasets using Splink and DuckDB

<Subtitle>Evaluating 1 billion record comparisons to deduplicate 8 million records in less than a minute</Subtitle>

## Summary

Data deduplication is a ubiquitous problem that results from multiple records being collected for the same entity:

| first_name | surname | dob        | city       | source_data_system |
| ---------- | ------- | ---------- | ---------- | ------------------ |
| lucas      | smith   | 1984-01-02 | London     | sales              |
| lucas      | smyth   | 1984-07-02 | Manchester | sales              |
| lucas      | smyth   | 1984-07-02 |            | marketing          |


The lack of a unique identifier means that there's no easy way of knowing whether these records refer to the same person.

[Splink](https://github.com/moj-analytical-services/splink) is a Python library for deduplicating and linking this kind of data. It's designed for use on very large datasets, so it's important that it is fast.

This blog post presents benchmarking results of Splink that show Splink is able to deduplicate an 8 million record dataset by evaluating 1 billion record comparisons in less than a minute, and at a cost of less than $1.00.

<VegaLite spec={summary_chart} />

To the best of my knowlege, these results show that it is fastest free tool for accurately deduplicating large datasets - by at least an order of magnitude. Please <Link to="/about/">let me know</Link> if you think otherwise!



## The data

A dataset of historical persons was collected from [WikiData](https://www.wikidata.org/wiki/Wikidata:Main_Page) using the [Wikidata Query Service](https://query.wikidata.org/).


## Methodology

Benchmarking was conducted on a variety of AWS EC2 instances using [pytest-benchmark](https://pypi.org/project/pytest-benchmark/).

When benchmarking the time taken to complete deduplication job, it's critical to report both:
- The number of input records
- The number of pairwise record comparisons made by the algorithm

User-specified blocking rules determine how many comparisons are made, and so it's always possible to get a better runtimes simply by using stricter blocking.   But blocking rules are carefully chosen, fewer comparisons comes at the cost of lower recall.

In this case, I've chosen to make a relatively large number of comparisons per input record to avoid the criticism that I'm using excessively tight blocking to artifiially reduce runtimes.

## Comparisons to other libraries

A chart of runtimes three other popular open source record linkage/data deudplication packages can be found in [this paper](https://imai.fas.harvard.edu/research/files/linkage.pdf) from the authors of the [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf) R package.

The chart shows fastLink has by far the best performance, at 400 minutes to deduplication 300,000 records on an 8 core machine.

Splink is able to deduplicate the same number of records in less than a minute on a comparable machine, suggesting it's at least two orders of magnitude faster.

A caveat is that these runtimes are not strictly comparable because the input datasets are not the same.  However, the size of the differences are so large that it's almost certain that Splink is a great deal faster.


## Accuracy

This post considers runtime only.  Accuracy statistics would only be valid on the specific datset used for benchmarking, and I do not have comparative accuracy statistics for other libraries.

However, there are theoretical and empircal reasons to believe Splink offers accuracy comparable to or better than alternaties:

- First, Splink is able to effectively harness most of the information in the input data in making a prediction - as explained in <Link to="/fellegi_sunter_accuracy/">this post</Link>.
- Second, an [independent empircal study](https://scholar.googleusercontent.com/scholar?q=cache:o4-c8w6DveYJ:scholar.google.com/+splink&hl=en&as_sdt=0,5&as_ylo=2023) have found Splink's accuracy compares favourably to alternatives.

I hope to write another post at some point looking at comparative accuracy in further details


[link off to accuracy blog posts here rather thank quoting accuracy]

## Accuracy

## Model details

<details>
<summary>Click to expand</summary>

```python

import splink

```

</details>


## Reproducuction materials

Materials to reproduce the results in this post can be found in the following two repos:
- [Code to set up AWS infrastructure](https://github.com/RobinL/benchmarking_splink_infrastructure)
- [Code to run the benchmarks](https://github.com/RobinL/run_splink_benchmarks_in_ec2)
