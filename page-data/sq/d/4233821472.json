{"data":{"allMarkdownRemark":{"nodes":[{"html":"<p>I'm sensing a vibe shift since the release of Opus 4.5 that it's becoming acceptable for devs in serious teams to write their code using LLMs. I'm expecting 2026 to see at least one more jump in capability equivalent to Opus 4.5, and writing serious code primarily using LLMs to become mainstream. The open questions will be more about etiquette and less about use, see <a href=\"https://www.robinlinacre.com/microblog/#using-ai-to-help-teammates-not-hinder-them\">here</a></p>","frontmatter":{"title":"LLMs becoming mainstream for serious teams","date":"2026-01-04","tags":["LLMs","Workflows"]}},{"html":"<p>Developers hate vibe coded PRs because they push the hard work of understanding the code onto the reviewer. My take: when working in a team, you should only use AI assistance if it makes your teammates' jobs easier not harder. Mostly this means you should be using AI to think more about what you're doing not less.</p>\n<p>I find a few patterns useful to stick to this rule:</p>\n<ol>\n<li>Vibe code one or more solutions on a temp branches. When you decide you like an approach, implement it yourself by hand on a new branch, step by step, deliberately seeking to understand every part of it. Consider splitting into multiple smaller PRs.</li>\n<li>Put in deliberate, additional effort prior to opening the PR. For example, get the best model available to you (e.g. GPT 5.2 Pro) to do review the diff prior to opening the PR. Read what it says carefully.</li>\n<li>In the PR description, write notes about any qualitative decisions you make, what you tried/thought about, and why you made your decisions.</li>\n</ol>\n<p>This all fits very cleaning into my pre-AI workflow, where I would always 'make it work, then make it good'. The 'make it work' part is essentially a research step to help you understand the problem. You can only 'make it good' if you understand the problem. You cannot judge whether it is good without understanding the problem and the solution.</p>\n<p>One thing I really like is I can now vibe code multiple solutions using different approaches to quickly see which pans out best.</p>","frontmatter":{"title":"Using AI to Help Teammates, Not Hinder Them","date":"2025-12-14","tags":["LLMs","Collaboration"]}},{"html":"<p>In the official Vega <a href=\"https://vega.github.io/vega/examples/force-directed-layout/\">force-directed example</a> it isn’t obvious how to wire up links when your data uses unique ids rather than array positions.</p>\n<p>The <a href=\"https://github.com/vega/vega/blob/3d9691aa9ec597ee9a831e8294351f5a2dafa85e/docs/data/miserables.json\">Les Mis example dataset</a> used by the example looks like this:</p>\n<pre><code class=\"language-json\">{\n  \"nodes\": [\n    {\"name\":\"Myriel\",\"group\":1,\"index\":0},\n    {\"name\":\"Napoleon\",\"group\":1,\"index\":1},\n    {\"name\":\"Mlle.Baptistine\",\"group\":1,\"index\":2}\n    …\n  ],\n  \"links\": [\n    {\"source\":1,\"target\":0,\"value\":1},\n    {\"source\":2,\"target\":0,\"value\":8},\n    {\"source\":3,\"target\":0,\"value\":10}\n    …\n  ]\n}\n</code></pre>\n<p>That is: <code>source</code>/<code>target</code> are array indices into <code>nodes</code>. This is not how most data arrives.</p>\n<p>I didn't find the <a href=\"https://vega.github.io/vega/docs/transforms/force/\">docs</a> to be very clear:</p>\n<blockquote>\n<p>“If an id field parameter is provided, it is used to related link objects and node objects. Otherwise, the source and target fields should provide indices into the array of node objects.”</p>\n</blockquote>\n<p>Here are four fully working examples in gists:</p>\n<ul>\n<li><a href=\"https://gist.github.com/RobinL/3a08784cf81291825887db7c4f12643c#file-spec_data_hardcoded-json\">Hardcoded indices</a></li>\n<li><a href=\"https://gist.github.com/RobinL/3a08784cf81291825887db7c4f12643c#file-spec_no_lookup_force_in_data_section-json\">Use unique ids, force transform in data</a></li>\n<li><a href=\"https://gist.github.com/RobinL/3a08784cf81291825887db7c4f12643c#file-spec_no_lookup_force_in_mark_section-json\">Use unique ids, force transform in mark</a></li>\n<li><a href=\"https://gist.github.com/RobinL/3a08784cf81291825887db7c4f12643c#file-spec_with_lookup-json\">Bonus: How to use a lookup to derive hardcoded indices</a></li>\n</ul>\n<p>My recommendation is:</p>\n<ul>\n<li>If you want to be able to drag the nodes as in the [Les Mis example]((<a href=\"https://vega.github.io/vega/examples/force-directed-layout/\">https://vega.github.io/vega/examples/force-directed-layout/</a>), you need the force transform in mark.  Unfortunately this breaks the 'open in vega editor' button with a <code>Uncaught TypeError: Converting circular structure to JSON</code> error.</li>\n<li>If you don't need to be able to drag the nodes, use the force transform in data, because this means the 'open in vega editor' button works</li>\n</ul>\n<p>The following summarises the differences</p>\n<h2>1) Hardcoded indices (closest to the example)</h2>\n<p>Links are numeric indices. No <code>id</code> accessor needed.</p>\n<pre><code class=\"language-json\">// link-data\n{ \"values\": [ { \"source\": 0, \"target\": 1, \"weight\": 50.78 } ] }\n\n// force (inside mark or data section)\n{ \"force\": \"link\", \"links\": \"link-data\",\n  \"distance\": {\"signal\": \"$linkDistance\"}\n}\n</code></pre>\n<p>Good for quick tests. Brittle if node order changes.</p>\n<hr>\n<h2>2) No lookup, force in the data section (string IDs)</h2>\n<p>Let the force write <code>x/y</code> onto data tuples and bind them in the mark. Provide an <code>id</code> accessor that returns your node’s unique key.</p>\n<pre><code class=\"language-json\">// node-data (tuples carry the id)\n\"values\": [\n  { \"group\": 1, \"match_id\": \"a\" },\n  { \"group\": 1, \"match_id\": \"b\" }\n],\n\"transform\": [\n  {\n    \"type\": \"force\",\n    \"forces\": [\n      { \"force\": \"link\", \"links\": \"link-data\", \"id\": \"match_id\",\n        \"distance\": {\"signal\": \"$linkDistance\"} }\n    ]\n  }\n]\n\n// link-data (string endpoints)\n\"values\": [ { \"source\": \"a\", \"target\": \"b\" } ]\n\n// node mark (tell the symbol where to go)\n\"encode\": { \"update\": { \"x\": {\"field\": \"x\"}, \"y\": {\"field\": \"y\"} } }\n</code></pre>\n<p>Here <code>id: \"match_id\"</code> is evaluated on node tuples (no <code>datum.</code>).</p>\n<h2>3) No lookup, force in the mark section (string IDs)</h2>\n<p>Force writes directly to mark items, so you don’t set <code>x</code>/<code>y</code> in <code>encode</code>. But the <code>id</code> accessor must read from the underlying datum:</p>\n<pre><code class=\"language-json\">// force inside the node mark\n{\n  \"type\": \"force\",\n  \"forces\": [\n    { \"force\": \"link\", \"links\": \"link-data\", \"id\": \"datum.match_id\",\n      \"distance\": {\"signal\": \"$linkDistance\"} }\n  ]\n}\n</code></pre>\n<p>Key difference: inside a mark you need <code>id: \"datum.match_id\"</code>.</p>\n<h2>4) Bonus: pre-map link IDs to indices with <code>lookup</code></h2>\n<p>If your links come with IDs and you’d rather keep the index-based link force, translate them first:</p>\n<pre><code class=\"language-json\">// on link-data\n{\n  \"type\": \"lookup\",\n  \"from\": \"node-data\", \"key\": \"match_id\",\n  \"fields\": [\"match_id_l\", \"match_id_r\"], \"as\": [\"source\", \"target\"]\n},\n{ \"type\": \"formula\", \"as\": \"source\", \"expr\": \"datum.source.index\" },\n{ \"type\": \"formula\", \"as\": \"target\", \"expr\": \"datum.target.index\" },\n{ \"type\": \"project\", \"fields\": [\"source\", \"target\", \"weight\"] }\n</code></pre>\n<p>Then use the standard link force with no <code>id</code> accessor.</p>","frontmatter":{"title":"Different ways of setting out data in a Vega force-directed layout","date":"2025-09-22","tags":["data"]}},{"html":"<p>gpt-5 codex, a set of unit tests plus a carefully written <code>AGENTS.md</code> feels like a bit of an inflection point where it often will write better code than me, and my job is more understanding what it's done, and less fixing it</p>","frontmatter":{"title":"An Inflection Point with GPT-5 Codex","date":"2025-09-20","tags":["LLMs"]}},{"html":"<p>A lot of the job of coding with LLMs is 'semantic bootstrapping': slowly narrowing down the prompt until it's precise enough to get what you want. I do this intentionally: start by getting it to write a simple script that approximates what you want, iterate, then put this in your context (e.g. <code>AGENTS.md</code>) as part of instructions to integrate this functionality into a larger codebase.</p>","frontmatter":{"title":"Semantic bootstrapping with LLMs","date":"2025-09-20","tags":["LLMs"]}},{"html":"<p>Built an app to illustrate composing and decomposing. Tried OpenAI Codex, Claude Code, and Gemini 2.5 Pro on AI studio for implementing this app.</p>\n<ol>\n<li>Claude is good but expensive. It cost >£5 in less than 2 hrs.</li>\n<li>Codex failed pretty quickly to follow the spec.</li>\n<li>Gemini 2.5 is around as good as Claude, possibly a little better, and currently free on AI studio.</li>\n</ol>\n<p>On Gemini I'm putting the whole app in context, and asking for 'precise instructions for an LLM to implement', then giving those to GPT-4.1 to implement in VS Code Copilot.</p>\n<p>Mainly illustrates the annoying tradeoff that the companies are currently having to navigate in their agentic coders between good results, and including too much context (which increases cost too much).</p>\n<p>App here:\n<a href=\"https://rupertlinacre.com/d3_regrouping_exchanging/\">https://rupertlinacre.com/d3_regrouping_exchanging/</a></p>\n<p>Code inc spec and step by step implementation plan here:\n<a href=\"https://github.com/RupertLinacre/d3_regrouping_exchanging\">https://github.com/RupertLinacre/d3_regrouping_exchanging</a></p>","frontmatter":{"title":"Comparing AI Coders for Composing and Decomposing","date":"2025-06-08","tags":["LLMs"]}},{"html":"<p>Codex feels like the future for a range of async tasks. Sitting back and watching it successfully prepare multiple small PRs to my website in parallel last night felt like a pretty big paradigm shift. It makes me want all my software (docs, sheets etc.) to have a textfile/diffable edit mode - and have something like codex be an agentic 'control centre' for a wide range of work. As it stands, this video is closest to how I think it's currently useful: <a href=\"https://openai.com/index/introducing-codex/?video=1084810891\">https://openai.com/index/introducing-codex/?video=1084810891</a></p>","frontmatter":{"title":"Codex as a Control Centre","date":"2025-06-04","tags":["LLMs"]}},{"html":"<p>Working in Jupyter or VS Code and want to edit an Altair chart in the Vega-Lite editor easily? Use:</p>\n<pre><code class=\"language-python\">import altair as alt\nalt.renderers.enable(\"browser\")\nchart.show()\n</code></pre>\n<p><code>chart.show()</code> will open the chart in your default web browser.</p>","frontmatter":{"title":"Editing Altair Charts in the Vega-Lite Editor","date":"2025-06-03","tags":["altair","vega-lite","vscode","jupyter"]}},{"html":"<p>Want to make a histogram directly in DuckDB? Use the built-in <code>histogram</code> and <code>equi_width_bins</code> functions:</p>\n<pre><code class=\"language-python\">import duckdb\n\nsql = \"\"\"\nWITH bins AS (\n    SELECT\n        UNNEST(\n            MAP_ENTRIES(\n                histogram(\n                    your_numeric_column,\n                    equi_width_bins(-5.0, 20.0, &#x3C;num_bins>, nice := true)\n                )\n            )\n        ) AS bin\n    FROM your_table\n)\nSELECT\n    bin.key   AS bin_upper_bound,\n    bin.value AS count_in_bin\nFROM bins\nORDER BY bin.key;\n\"\"\"\n\n\nhistogram_df = duckdb.sql(sql)\n\n</code></pre>","frontmatter":{"title":"Making Histograms with DuckDB SQL","date":"2025-05-21","tags":["duckdb"]}},{"html":"<p>Need to strip suffixes on the fly? DuckDB’s COLUMNS star-expression can do it with one regex:</p>\n<pre><code class=\"language-python\">sql = \"\"\"\nSELECT COLUMNS('^(.*)_(cen)$') AS '\\\\1'\nFROM df\n\"\"\"\nduckdb.sql(sql).show(max_width=10000)\n</code></pre>\n<p>This removes the suffix <code>_cen</code> from all columns, returning only columns with the suffix.</p>\n<pre><code class=\"language-python\">sql = \"\"\"\nSELECT COLUMNS('^(.+?)_cen$|(.+)$') AS '\\\\1\\\\2'\nFROM your_table;\nFROM df\n\"\"\"\nduckdb.sql(sql).show(max_width=10000)\n</code></pre>\n<p>This removes the suffix <code>_cen</code> from any column name that has it, returning all columns (the names of cols without the suffix are unaffected).</p>\n<p>See Star Expressions → COLUMNS in the DuckDB docs: <a href=\"https://duckdb.org/docs/sql/query_syntax/select.html#star-expressions\">https://duckdb.org/docs/sql/query_syntax/select.html#star-expressions</a></p>","frontmatter":{"title":"Regex-Based Column Renaming in DuckDB","date":"2025-05-21","tags":["duckdb"]}},{"html":"<p>If you step back, it's kind of weird that there's no mainstream programming language that has tables as first class citizens. Instead, we're stuck learning multiple APIs (polars, pandas) which are effectively programming languages for tables.</p>\n<p>R is perhaps the closest, because it has <code>data.table</code> as a 'first class citizen', but most people don't seem to use it, and use e.g. <code>tibble</code>s from dplyr instead.</p>\n<p>The root cause seems to be that we still haven't figured out the best language to use to manipulate tabular data yet.  It feels like there's been some convergence on some common ideas. Polars is kindof similar to dplyr.  But no standard, except perhaps SQL.</p>","frontmatter":{"title":"Why aren't tables first class citizens in programming languages?","date":"2025-04-23","tags":["data"]}},{"html":"<p>Open sourcing code in government has become immensely more valuable in the LLM era because it means there is no sensitivity around applying the best AI models to understand or improve the code. I'm finding o4-mini incredible at searching and explaining entire repos, and identifying which files do what, just by pasting the URL and asking questions.</p>","frontmatter":{"title":"Open Sourcing Code in Government in the LLM Era","date":"2025-04-22","tags":["LLMs"]}},{"html":"<p>AI hypothetical: If you assigned a motivated person with an IQ of 200 to a random civil service position, would they be exceptionally productive, they were given all available recorded context?   I suspect the returns to intelligence would be relatively modest at the moment.</p>\n<p>This seems quite relevant to question of how quickly can AI speed up progress in the short term.  At the moment, based on this sort of example, I tend to agree with the ideas of Ege Erdil and Tamay Besiroglu e.g. <a href=\"https://www.dwarkesh.com/p/ege-tamay#:~:text=by%20better%20management%3F-,Ege%20Erdil%2001%3A31%3A05,-But%20that%E2%80%99s%20not\">here</a> and Tyler Cowen <a href=\"https://www.dwarkesh.com/p/tyler-cowen-4\">here</a>.</p>","frontmatter":{"title":"AI hypothetical in the Civil Service","date":"2025-04-20","tags":["LLMs"]}},{"html":"<p><a href=\"https://opentimes.org/\">OpenTimes</a> are using a clever combination of duckdb and <code>.parquet</code> files to serve huge datasets to a web frontend.</p>\n<p>See <a href=\"https://sno.ws/opentimes/\">this blog</a> from Dan Snow:</p>\n<blockquote>\n<p>The entire OpenTimes backend is just static Parquet files on Cloudflare’s R2. There’s no RDBMS or running service, just files and a CDN. The whole thing costs about $10/month to host and costs nothing to serve. In my opinion, this is a great way to serve infrequently updated, large public datasets at low cost (as long as you partition the files correctly).</p>\n</blockquote>\n<blockquote>\n<p>The query layer uses a single DuckDB database file with views that point to static Parquet files via HTTP. This lets you query a table with hundreds of billions of records after downloading just the ~5MB pointer file.</p>\n</blockquote>\n<p>To dig into this a bit deeper we can attach their database at <code>https://data.opentimes.org/databases/0.0.1.duckdb</code>:</p>\n<p>And see that the 'tables' are just reference to very large numbers of parquet files:</p>\n<pre><code class=\"language-python\">import duckdb\n\nduckdb.execute(\"ATTACH 'https://data.opentimes.org/databases/0.0.1.duckdb'\")\n\nsql = \"\"\"\nSELECT sql\nFROM duckdb_views()\nwhere schema_name = 'public'\n\n\"\"\"\n\nduckdb.sql(sql).fetchone()[0]\n</code></pre>\n<p>returns:</p>\n<pre><code class=\"language-sql\">CREATE VIEW public.points AS\nSELECT * FROM\nread_parquet(\nmain.list_value(\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=01/points-0.0.1-car-2020-state-01-0.parquet',\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=02/points-0.0.1-car-2020-state-02-0.parquet',\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=04/points-0.0.1-car-2020-state-04-0.parquet',\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=05/points-0.0.1-car-2020-state-05-0.parquet',\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=06/points-0.0.1-car-2020-state-06-0.parquet',\n'https://data.opentimes.org/points/version=0.0.1/mode=car/year=2020/geography=state/state=08/points-0.0.1-car-2020-state-08-0.parquet',\n...700 of these!\n)\n)\n</code></pre>\n<p>More info from Simon W <a href=\"https://news.ycombinator.com/item?id=43393163\">here</a> and <a href=\"https://simonwillison.net/2025/Mar/17/opentimes/\">here</a>.</p>","frontmatter":{"title":"Opentimes and a clever use of DuckDB","date":"2025-04-08","tags":["duckdb"]}},{"html":"<p>I see too much focus on trying to find applications of LLMs to help other people 'at scale' with their jobs. At the moment, the output of LLMs is rarely useful for business rules or passive consumption. The lower hanging fruit is encouraging and helping people use AI directly, interactively, and however they see fit for their own jobs.</p>\n<p><em>Addendum 2025-04-07</em>: One thing in particular you want to avoid is giving LLM outputs to people who don't use LLMs.  I suspect this is why Apple Intelligence has done so badly thus far.  If your user is experienced with LLMs and is expecting it, it's fine to give them LLM outputs because they will treat them with the appropriate degree of scepticism.  Otherwise, they could be misled.</p>","frontmatter":{"title":"AI Upskilling Over AI Engineering","date":"2025-04-04","tags":["LLMs"]}},{"html":"<p>Here's a simple workflow for transcribing podcasts using Gemini 2.5 Pro in <a href=\"https://aistudio.google.com/\">Google AI Studio</a>. Initial tests suggest it works very well.</p>\n<ol>\n<li>Find the direct MP3 link to your podcast using <a href=\"https://getrssfeed.com/\">GetRSSFeed</a></li>\n<li>Drag the MP3 file directly into Gemini 2.5 Pro</li>\n<li>Use a simple prompt like \"transcribe this\" with the following structured output specification (click 'Structured output' then 'edit')</li>\n</ol>\n<pre><code class=\"language-json\">{\n  \"type\": \"object\",\n  \"properties\": {\n    \"items\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"timestamp\": {\n            \"type\": \"string\",\n            \"description\": \"mm:ss\"\n          },\n          \"text\": {\n            \"type\": \"string\"\n          },\n          \"speaker_name\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\n          \"timestamp\",\n          \"text\",\n          \"speaker_name\"\n        ]\n      }\n    }\n  },\n  \"required\": [\n    \"items\"\n  ]\n}\n</code></pre>\n<p>This gives you a nicely formatted transcript with timestamps and speaker identification - no special tools needed beyond Gemini 2.5 Pro's multimodal capabilities.</p>","frontmatter":{"title":"Easily Transcribe Podcasts with Gemini 2.5 Pro","date":"2025-04-02","tags":["LLMs","audio","productivity"]}},{"html":"<p>Today I learned you can combine the VS Code debugger with the Jupyter interactive window enabling debugging with rich output.</p>\n<p><strong>My Workflow</strong>: I use plain <code>.py</code> files in VS Code, executing \"cells\" by selecting code and pressing Shift+Enter to send it to the Jupyter (ipykernel) interactive window.</p>\n<p>Previously, I found it frustrating having to choose between:</p>\n<ol>\n<li>Sending code to the interactive window to get rich output (pandas tables, charts, etc.)</li>\n<li>Setting a debug point to explore variables interactively</li>\n</ol>\n<p>Today I discovered you can have both! Use the <strong>Jupyter: Debug current file in interactive window</strong> command.</p>\n<p>At first, this doesn't seem to do anything special - you still execute debug commands in the debug console, making it appear no different from standard debugging.</p>\n<p>But if you wrap any command in <code>display()</code>, the output also appears in the interactive window with full rich formatting.</p>\n<p>This gives you the best of both worlds: interactive debugging with the ability to visualize complex data structures and plots in the same workflow.</p>","frontmatter":{"title":"Combining VS Code Debugger with Jupyter Interactive Window","date":"2025-04-02","tags":["python","jupyter","vscode","debugging","productivity"]}},{"html":"<p>Want to retain one row per unique value of a certain column or columns?  Use this:</p>\n<pre><code class=\"language-sql\">SELECT DISTINCT ON (column1) *\nFROM your_table\n</code></pre>","frontmatter":{"title":"DuckDB Dedupe","date":"2025-04-01","tags":["duckdb"]}},{"html":"<p>If you're using duckdb in a python script or jupyter notebook, you can run <code>con.execute('CALL start_ui()')</code> at any point, and the ui will pop right up in your web browser with the current database automatically available.</p>\n<p>(I knew about the UI, but I had missed this trick!)</p>","frontmatter":{"title":"Quick DuckDB UI Access in Python","date":"2025-04-01","tags":["duckdb","python","jupyter"]}},{"html":"<p><a href=\"https://pure.md\">pure.md</a> turns webpages into markdown files, making it easy to paste web content as context for an LLM, see <a href=\"https://news.ycombinator.com/item?id=43528339\">this post</a></p>","frontmatter":{"title":"pure.md: A CDN for LLMs to access web content","date":"2025-04-01","tags":["LLMs"]}},{"html":"<p>Want percentages and the total?  Use <code>group by cube</code>:</p>\n<pre><code class=\"language-python\">import duckdb\n\nNUM_ROWS = 1000\n\nduckdb.sql(f\"\"\"\nCREATE OR REPLACE TABLE array_dataset AS\nWITH fruits AS (\n    SELECT ['apple', 'banana', 'pear', 'plum'] AS fruit_list\n),\nrandom_fruits AS (\n    SELECT\n        list_extract((SELECT fruit_list FROM fruits), 1 + (random() * 3)::INTEGER) AS fruit\n    FROM range({NUM_ROWS})\n)\nSELECT * FROM random_fruits\n\"\"\")\n\nduckdb.table(\"array_dataset\").show()\n\n\nresult = duckdb.sql(\"\"\"\nWITH fruit_counts AS (\n  SELECT\n    fruit,\n    COUNT(*) AS count\n  FROM array_dataset\n  GROUP BY CUBE(fruit)\n),\ntotal_count AS (\n  SELECT count FROM fruit_counts WHERE fruit IS NULL\n)\nSELECT\n  COALESCE(fruit, 'TOTAL') AS fruit,\n  count,\n  FORMAT('{:.2f}%%', 100.0 * count / (SELECT count FROM total_count)) AS percentage\nFROM fruit_counts\nORDER BY fruit = 'TOTAL', fruit\n\"\"\")\nresult.show()\n\n</code></pre>","frontmatter":{"title":"DuckDB Cube","date":"2025-03-27","tags":["duckdb"]}},{"html":"<p>When working with complex nested types such as structs, I often want to try syntax out, but it can be hard to figure out the syntax to write the literal value.</p>\n<p>You can do this fairly easily with the duckdb CLI using insert model (web shell does not work).  See <a href=\"https://duckdb.org/docs/stable/clients/cli/output_formats.html\">output formats</a>.</p>\n<pre><code class=\"language-bash\">duckdb\n.mode insert\n</code></pre>\n<p>Now if I write any SQL statement, it will return the output in the format of a SQL <code>INSERT</code> statement - i.e how to write the literal value.</p>\n<p>Example:</p>\n<pre><code class=\"language-sql\">SELECT ARRAY[struct_pack(key1 := 'value1', key2 := 42::float),struct_pack(key1 := 'value1', key2 := 43.2::float)] AS s;\n</code></pre>\n<p>returns:</p>\n<pre><code class=\"language-sql\">INSERT INTO \"table\"(s) VALUES('[{''key1'': value1, ''key2'': 42}, {''key1'': value1, ''key2'': 43}]');\n</code></pre>\n<p>Of course, you could also <code>SELECT</code> from the first row of an existing dataframe to see how to create literals for each column.</p>","frontmatter":{"title":"DuckDB Literals","date":"2025-03-27","tags":["duckdb"]}},{"html":"<ul>\n<li>Progress is enormously faster if you can work on micro-libraries that fit easily into context.  Split out bits of functionality into separate libraries more often than you would if you weren't using an LLM.</li>\n<li>When implementing a new feature, first ask the LLM to perform an architectural review of the whole codebase to see whether a refactor is needed that will make the new feature easier to implement</li>\n<li>Regularly give the LLM the whole codebase and ask it whether there's any used code or functions.  They often leave a mess by accident when refactoring</li>\n<li>Have a set of tests or e.g. a demo webpage page (written by the LLM) that give you immediate feedback on whether the library is working.  The idea is to have a source of information you can easily copy and paste back into the LLM to help it fix bugs</li>\n<li>Regularly ask the LLM whether the codebase currently seems healthy and whether there's a way to achieve the same thing in a simpler or clearer fashion</li>\n</ul>","frontmatter":{"title":"AI coding tips","date":"2025-03-11","tags":["LLMs"]}},{"html":"<p>Weird experience tonight teaching o3-mini to debug. It can code better than me, but seems clueless at debugging. 'Don't try and fix this all at once - create a minimal reprex that outputs debugging information that will help you solve this problem'. Only then does it succeed</p>\n<p>Feels like a combination of</p>\n<ol>\n<li>asking for clarification</li>\n<li>knowing how to debug</li>\n<li>adding code interpreter</li>\n</ol>\n<p>would make Cusror far more powerful, without any significant breakthroughs needed in underlying LLM capability. Knowing when to do (1) is perhaps hardest</p>","frontmatter":{"title":"Teaching AI models to debug","date":"2025-02-19","tags":["ai","debugging"]}},{"html":"<p>Why don't AI models ask for clarification more often? I've often under-specified a question and can remember very few times the model has asked for more info before proceeding. Only later do I realise I didn't provide enough context,</p>\n<p>Feels potentially hard to get data for training/RL but once or twice I've gone into a 'doom loop' of failed code suggestions, when zooming out a bit the problem was I'd asked for a specific approach which was never going to work, and the model one-shotted it as soon as it realised the intent and that the direction was wrong.</p>","frontmatter":{"title":"Why don't AI models ask for clarification?","date":"2025-02-17","tags":["ai","LLMs"]}},{"html":"<p>From <a href=\"https://news.ycombinator.com/item?id=43097209\">epistasis</a>:</p>\n<blockquote>\n<p>One other key part of this is freezing a timestamp with your dependency list, because Python packages are absolutely terrible at maintaining compatibility a year or three or five later as PyPI populates with newer and newer versions. The special toml incantation is [tool.uv] exclude-newer:</p>\n<pre><code># /// script\n# dependencies = [\n#   \"requests\",\n# ]\n# [tool.uv]\n# exclude-newer = \"2023-10-16T00:00:00Z\"\n# ///\n</code></pre>\n<p><a href=\"https://docs.astral.sh/uv/guides/scripts/#improving-reproducibility\">https://docs.astral.sh/uv/guides/scripts/#improving-reproducibility</a>\nThis has also let me easily reconstruct some older environments in less than a minute, when I've been version hunting for 30-60 minutes in the past. The speed of uv environment building helps a ton too.</p>\n</blockquote>","frontmatter":{"title":"Using uv timestamp for dependency management","date":"2025-02-16","tags":["python"]}},{"html":"<p>I think the single most productivity-enhancing use of LLMs in government would be to give all government devs and data scientists access to Cursor (or similar). I am not yet convinced of the widespread value of 'behind the scenes' use cases of LLMs, but very bullish on skilled human-in-the-loop applications, especially coding.</p>\n<p>Undoubtedly this situation will change as models improve, but at the moment there's usually not enough 9s of reliability to use in fully automated use cases.</p>","frontmatter":{"title":"LLMs for government productivity","date":"2025-02-13","tags":["LLMs"]}},{"html":"<p><a href=\"https://lexfridman.com/deepseek-dylan-patel-nathan-lambert-transcript/#:~:text=It%E2%80%99s%20even%20less%20prevalent\">Latest Lex episode at 02:43:31</a> has a good section on why chain-of-thought matters over and above just 'it increases the benchmark results'.</p>\n<p>It hit me very similar to when I <a href=\"https://youtu.be/U5OD8MjYnOM?si=J9rACnXmZH2HDC9R&#x26;t=6055\">first heard</a> (<a href=\"https://v1.transcript.lol/read/youtube/%40lexfridman/6522f348033150beacd16e2a?part=1#:~:text=And%20I%20think%20there%27s%20a%20lot%20of%20fundamental%20questions\">transcript</a>) the idea that code generation abilities would improve faster than natural language. It feels like potentially chain-of-thought is the key to making this work.</p>\n<p>These two Kapathy tweets also very relevant:\n<a href=\"https://x.com/karpathy/status/1883941452738355376\">tweet1</a> <a href=\"https://x.com/karpathy/status/1885026028428681698\">tweet2</a></p>\n<p>Seems quite likely we'll see superhuman coding abilities in the not too distant future.</p>","frontmatter":{"title":"LLMs getting better at coding","date":"2025-02-05","tags":["LLMs"]}},{"html":"<p>You can use DuckDB in ChatGPT's code interpreter by providing this specific wheel file:</p>\n<p><code>duckdb-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</code></p>\n<p>from <a href=\"https://pypi.org/project/duckdb/#files\">here</a>.</p>\n<p>If you encounter any issues, you can copy paste the available wheels into ChatGPT and ask it to analyze which is the most likely to work.</p>\n<p>For Altair, you need:\n<a href=\"https://files.pythonhosted.org/packages/68/0e/882f7c0e073bf1f310dce159af6186826ca9b8ee7c170771c23e52a373dc/narwhals-1.24.1-py3-none-any.whl\">narwhals-1.24.1-py3-none-any.whl</a> then\n<a href=\"https://files.pythonhosted.org/packages/aa/f3/0b6ced594e51cc95d8c1fc1640d3623770d01e4969d29c0bd09945fafefa/altair-5.5.0-py3-none-any.whl\">altair-5.5.0-py3-none-any.whl</a></p>\n<p>Note: I could not get png generation to work because the <code>vl_convert_python</code> wheel would not install.</p>","frontmatter":{"title":"Using DuckDB in ChatGPT Code Interpreter","date":"2025-01-28","tags":["duckdb","LLMs"]}},{"html":"<p>Rob Allen says:</p>\n<blockquote>\n<p>I create a fair few scripts in my ~/bin/ directory to automate tasks. Since discovering <a href=\"https://akrabat.com/defining-python-dependencies-at-the-top-of-the-file/\">uv and inline script metadata</a>, I've started using Python far more for these.</p>\n<p>As <code>~/bin</code> is on my path, I want to run the script by calling it directly on the command line. To do this, I use this shebang:</p>\n<pre><code class=\"language-bash\">#!/usr/bin/env -S uv run --script\n</code></pre>\n</blockquote>\n<p>Full article <a href=\"https://akrabat.com/using-uv-as-your-shebang-line/\">here</a>.</p>","frontmatter":{"title":"Using a uv shebang line","date":"2025-01-28","tags":["python"]}},{"html":"<p>The <a href=\"https://clickhouse.com/docs/en/operations/utilities/clickhouse-obfuscator\">ClickHouse obfuscator</a> is a tool for anonymizing production data while preserving its key statistical properties. It maintains:</p>\n<ul>\n<li>Value cardinalities and distributions</li>\n<li>Data compression ratios</li>\n<li>String lengths and UTF-8 validity</li>\n<li>Time series continuity</li>\n</ul>\n<p>See also <a href=\"https://github.com/ClickHouse/ClickHouse/tree/master/programs/obfuscator\">here</a>.</p>","frontmatter":{"title":"ClickHouse Obfuscator - Data Anonymization Tool","date":"2025-01-16","tags":["data"]}},{"html":"<p>Three key priorities for building a National Data Library:</p>\n<ol>\n<li>\n<p>Data sharing and governance must come first - before any technical solutions. Success depends on data owners being convinced the NDL will make their lives easier, not harder.</p>\n</li>\n<li>\n<p>Access permissions and management systems need to be rock-solid before building analytical capabilities. Get the foundations right.</p>\n</li>\n<li>\n<p>Design for an AI-first future - by the time the NDL is delivered, most analysis will likely be AI-assisted. The architecture needs to anticipate this.</p>\n</li>\n</ol>","frontmatter":{"title":"National Data Library thoughts","date":"2025-01-16","tags":["data"]}},{"html":"<p>DuckDB offers special connection types in Python:</p>\n<ul>\n<li><code>:memory:name</code> - Creates/connects to a named in-memory database that can be shared across connections</li>\n<li><code>:default:</code> - Uses the default connection stored in the DuckDB module</li>\n</ul>\n<p>Example:</p>\n<pre><code class=\"language-python\">import duckdb\n\n# Create table in default connection\nduckdb.execute(\"CREATE TABLE tbl AS SELECT 42 as value\")\n\n# Access same table through explicit default connection\ncon = duckdb.connect(\":default:\")\ncon.sql(\"SELECT * FROM tbl\")  # Works!\n\n# Shared named memory connection\ncon3 = duckdb.connect(\":memory:shared_db\")\ncon4 = duckdb.connect(\":memory:shared_db\")  # Same database as con3\n</code></pre>\n<p>See docs <a href=\"https://duckdb.org/docs/api/python/dbapi.html\">here</a></p>","frontmatter":{"title":"Understanding DuckDB Connection Types in Python","date":"2024-01-16","tags":["python","duckdb","database"]}},{"html":"<p>When working with Jupyter notebooks in VS Code, add these settings to your <code>.vscode/settings.json</code>:</p>\n<pre><code class=\"language-json\">{\n    \"jupyter.notebookFileRoot\": \"${workspaceFolder}\",\n    \"python.analysis.extraPaths\": [\n        \"${workspaceFolder}\"\n    ]\n}\n</code></pre>\n<p>Now, the interactive window will run any script as if it's running from the root directory even if it's nested.</p>","frontmatter":{"title":"Configuring Python Path Visibility in VS Code interactive window (Jupyter)","date":"2024-01-16","tags":["vscode","jupyter","tips"]}},{"html":"<p>Quote I provided for One Big Thing for how the analytical profession is changing:</p>\n<blockquote>\n<p>Large Language Models democratise and superpower open source, code-based data tools, resulting in higher quality and more transparent.</p>\n</blockquote>","frontmatter":{"title":"How analytical profession will change","date":"2023-07-12","tags":["LLMs"]}},{"html":"<p>The following is an extract from an email I sent in April 2023 - of interest because it feels like this situation hasn't changed dramatically in the intervening 2 years.</p>\n<p>--</p>\n<p>No doubt you've seen some of the media coverage around new AI models, especially OpenAI GPT models, and have been thinking about how to use them with your teams.</p>\n<p>I've been using them quite extensively, and the more I use them, the more impressed I am - i.e. I'm pretty sure this is not just a fad, but is a serious technological breakthrough that has the potential to revolutionise quite significant parts of Civil Service work.</p>\n<p>I thought it may be useful to note a handful of the areas where it feels like there is low hanging fruit where the models may be appropriate for serious applications:</p>\n<ol>\n<li>\n<p><strong>Zero shot labelling.</strong> This is an ability to take an input document and categorise it according to any criterion of your choice without training a new AI model (that's the 'zero shot' bit). For example, taking a sentencing transcript as an input and the model categorising whether there was 'use of a weapon'. The important technical advance here is that these new models understand semantics, not just keywords, so the transcript could contain 'the victim was stabbed', and the computer would recognise this as use of a weapon.</p>\n</li>\n<li>\n<p><strong>Semantic search across a large corpus of potentially unstructured documents.</strong> There's an example here of using these models to analyse 1000 pages of Tesla's annual reports: <a href=\"https://twitter.com/mayowaoshin/status/1640385062708424708\">Tweet</a>.</p>\n</li>\n</ol>\n<p>Both (1) and (2) have been 'somewhat' possible in the past, but have been lots of work and haven't worked that well. What's new is that these are now much easier and more accurate.</p>\n<ol start=\"3\">\n<li><strong>Code completion.</strong> As a data scientist, I'm getting ChatGPT 4 to write probably 50%+ of my code. So at the very least, these models are a huge productivity amplifier to data scientists.</li>\n</ol>\n<p>The biggest challenge is around data sharing and using these tools with sensitive government data. It feels like getting a head start on understanding these legal issues may be an important first step.</p>","frontmatter":{"title":"An early email I sent on LLMs","date":"2023-04-16","tags":["LLMs"]}},{"html":"<p>Seems likely every child will soon have access to an always-on AI personal tutor for every subject (using tech like ChatGPT). Makes me wonder what skills will be most important for (my) kids to have - intrinsic motivation and self control to turn off distractions seem critical</p>","frontmatter":{"title":"Future Skills in an AI-Tutored World","date":"2023-01-23","tags":["LLMs"]}}]}}}